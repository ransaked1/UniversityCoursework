{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f16dda49",
   "metadata": {},
   "source": [
    "Sources:\n",
    "https://matplotlib.org/stable/tutorials/pyplot.html\n",
    "https://pypi.org/project/langdetect/\n",
    "https://www.geeksforgeeks.org/adding-value-labels-on-a-matplotlib-bar-chart/\n",
    "https://neptune.ai/blog/vectorization-techniques-in-nlp-guide\n",
    "https://scikit-learn.org/0.21/documentation.html\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e5b2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misc imports\n",
    "import pipreqsnb\n",
    "import re\n",
    "import os\n",
    "import demoji\n",
    "\n",
    "# ML imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# from imports\n",
    "from deep_translator import GoogleTranslator\n",
    "from langdetect import detect_langs\n",
    "from tqdm.notebook import tqdm\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fd6540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate requirements.txt\n",
    "!pipreqsnb --force --use-local Source_code.ipynb\n",
    "\n",
    "# Remove version numbers and duplicates\n",
    "name = 'requirements.txt'\n",
    "\n",
    "with open(name, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "packages = set()\n",
    "\n",
    "for line in lines:\n",
    "    packageName, _ = line.split('==', 1)\n",
    "    packages.add(packageName)\n",
    "\n",
    "with open(name, 'w') as file:\n",
    "    for packageName in packages:\n",
    "        file.write(f'{packageName}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e476716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import datasets\n",
    "trainingData = pd.read_table(\"mediaeval-2015-trainingset.txt\", sep=\"\\t\",lineterminator='\\n', skiprows=(0),  header=(0))\n",
    "testingData = pd.read_table(\"mediaeval-2015-testset.txt\", sep=\"\\t\", lineterminator='\\n', skiprows=(0),  header=(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8b8e70",
   "metadata": {},
   "source": [
    "# Data Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c9d57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9a771b",
   "metadata": {},
   "outputs": [],
   "source": [
    "testingData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9341454b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"<----------------- TRAINING ----------------->\\n\")\n",
    "trainingData.info()\n",
    "print(\"\\n\\n<----------------- TESTING ----------------->\\n\")\n",
    "testingData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccf9a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData['tweetText'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7740b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "testingData['tweetText'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96f2bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real labels in training\n",
    "print(\"Real count:\", trainingData['label'].isin(['real']).sum(axis=0))\n",
    "# Fake labels in training\n",
    "print(\"Fake count:\", trainingData['label'].isin(['fake', 'humor']).sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b59148",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "Columns: tweetId, tweetText, userId, imageId(s), username, timestamp, label<br>\n",
    "\n",
    "### Data quality:\n",
    "1. Tweets end in URLs\n",
    "2. Emojis and special characters\n",
    "3. Diverse languages (mixed languages)\n",
    "4. Text with typos\n",
    "5. All features non-null and strings\n",
    "\n",
    "### Data bias:\n",
    "1. 1901 duplicates in training\n",
    "2. 49 duplicates in testing\n",
    "3. 65.5% fake\n",
    "\n",
    "### Training Data:\n",
    "Size: 14277 entries<br>\n",
    "Unique tweetText: 12376 entries<br>\n",
    "Real count: 4921<br>\n",
    "Fake count: 9356\n",
    "\n",
    "### Testing Data:\n",
    "Size: 3755 entries<br>\n",
    "Unique tweetText: 3706 entries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e2f7f6",
   "metadata": {},
   "source": [
    "# Data Visualisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c57b67",
   "metadata": {},
   "source": [
    "## Fake vs Real Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee17d399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map 'humor' label to 'fake'\n",
    "trainingData['label'] = trainingData['label'].replace('humor', 'fake')\n",
    "\n",
    "# Plotting the tweet counts by label for the training dataset\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "trainingData['label'].value_counts().plot(kind='bar', color=['blue', 'red'])\n",
    "plt.title('Tweet Counts by Label (Training Data)')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Tweet Count')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.savefig('figures/CountsByLabelTraining.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12410c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the tweet counts by label for the testing dataset\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "testingData['label'].value_counts().plot(kind='bar', color=['blue', 'red', 'green'])\n",
    "plt.title('Tweet Counts by Label (Testing Data)')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Tweet Count')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.savefig('figures/CountsByLabelTesting.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f9e4cc",
   "metadata": {},
   "source": [
    "## Tweet Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250f62ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the length of each tweet and add it as a new column\n",
    "trainingData['tweetLength'] = trainingData['tweetText'].apply(len)\n",
    "testingData['tweetLength'] = testingData['tweetText'].apply(len)\n",
    "\n",
    "# Plot the distribution of tweet lengths\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(trainingData['tweetLength'], bins=5000, color='black')\n",
    "plt.title('Distribution of Tweet Lengths (Training Data)')\n",
    "plt.xlabel('Tweet Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig('figures/TweetLength.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dba356b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out tweets longer than 150 characters\n",
    "trainingDataFiltered = trainingData[trainingData['tweetLength'] <= 150]\n",
    "\n",
    "# Plot the distribution of filtered tweet lengths\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(trainingDataFiltered['tweetLength'], bins=50, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Tweet Lengths (Training Data)')\n",
    "plt.xlabel('Tweet Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig('figures/TweetLength150.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1ccc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get string greater than 500 characters\n",
    "longTweet = trainingData[trainingData['tweetLength'] > 500].head(1)\n",
    "\n",
    "# Print the full text of the identified tweet\n",
    "for _, tweet in longTweet.iterrows():\n",
    "    print(f\"Tweet ID: {tweet['tweetId']}\")\n",
    "    print(f\"Tweet Text:\\n{tweet['tweetText']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350e2c8a",
   "metadata": {},
   "source": [
    "## Language Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4972d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text\n",
    "def cleanText(text):\n",
    "    # Remove emojis\n",
    "    text = re.sub(r'\\\\u[0-9a-fA-F]+', '', text)\n",
    "    \n",
    "    # Remove non-ASCII characters\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    \n",
    "    # Remove links\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Remove hashtags\n",
    "    text = re.sub(r'#\\S+', '', text)\n",
    "    \n",
    "    # Remove tags\n",
    "    text = re.sub(r'@\\S+', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "trainingData['tweetTextClean'] = trainingData['tweetText'].apply(cleanText)\n",
    "testingData['tweetTextClean'] = testingData['tweetText'].apply(cleanText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14d8552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language detection function using langdetect from https://pypi.org/project/langdetect/\n",
    "def languageDetection(text):\n",
    "    try:\n",
    "        # If text is too short mark it as error\n",
    "        if len(text) < 3:\n",
    "            return 'err', 0.0\n",
    "            \n",
    "        langs = detect_langs(text) \n",
    "        for item in langs: \n",
    "            return item.lang, item.prob\n",
    "        \n",
    "        # Error if no language detected\n",
    "        return 'err', 0.0\n",
    "    except Exception as e:\n",
    "        return 'err', 0.0\n",
    "    \n",
    "tqdm.pandas(desc=\"Language Detection Training\")\n",
    "trainingData[['language', 'confidence']] = trainingData['tweetTextClean'].progress_apply(lambda x: pd.Series(languageDetection(x), dtype=object))\n",
    "\n",
    "tqdm.pandas(desc=\"Language Detection Testing\")\n",
    "testingData[['language', 'confidence']] = testingData['tweetTextClean'].progress_apply(lambda x: pd.Series(languageDetection(x), dtype=object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649c649d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the language distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "languageCounts = trainingData['language'].value_counts()\n",
    "\n",
    "languageCounts.plot(kind='bar', color='skyblue')\n",
    "plt.title('Language Distribution in Training Data')\n",
    "plt.xlabel('Language')\n",
    "plt.ylabel('Tweet Count')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.savefig('figures/LanguageDistribution.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718f626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [i/10 for i in range(11)]  # Bins from 0.0 to 1.0 in increments of 0.1\n",
    "\n",
    "# Cut the into bins\n",
    "trainingData['confidenceBin'] = pd.cut(trainingData['confidence'], bins=bins, include_lowest=True)\n",
    "testingData['confidenceBin'] = pd.cut(testingData['confidence'], bins=bins, include_lowest=True)\n",
    "\n",
    "# Count tweets in bins\n",
    "confidenceCounts = trainingData['confidenceBin'].value_counts().sort_index()\n",
    "\n",
    "print(\"Count of tweets for each confidence bin:\")\n",
    "print(confidenceCounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdf48b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store samples for each confidence bin\n",
    "samplesByBin = {}\n",
    "\n",
    "# Get 5 random texts from bin\n",
    "def getSamples(group):\n",
    "    # Check if the group has enough samples to take 5 random samples\n",
    "    if len(group) >= 5:\n",
    "        return group.sample(5)\n",
    "    else:\n",
    "        return group\n",
    "\n",
    "for value, group in trainingData.groupby('confidenceBin'):\n",
    "    samplesByBin[value] = getSamples(group)\n",
    "\n",
    "# Display samples\n",
    "for binValue, samples in samplesByBin.items():\n",
    "    print(f\"\\nConfidence Bin: {binValue}\")\n",
    "    for index, row in samples.iterrows():\n",
    "        print(f\"Language: {row['language']}, Text: {row['tweetTextClean']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4449513",
   "metadata": {},
   "source": [
    "## Timestamp Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66dfe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the timestamp to datetime\n",
    "tqdm.pandas(desc=\"Converting timestamps\")\n",
    "trainingData['timestamp'] = trainingData['timestamp'].progress_apply(lambda x: pd.to_datetime(x.replace(' ', ''), format=\"%a%b%d%H:%M:%S+0000%Y\"))\n",
    "testingData['timestamp'] = testingData['timestamp'].progress_apply(lambda x: pd.to_datetime(x.replace(' ', ''), format=\"%a%b%d%H:%M:%S+0000%Y\"))\n",
    "\n",
    "# Extract components into columns\n",
    "trainingData['year'] = trainingData['timestamp'].dt.year\n",
    "trainingData['month'] = trainingData['timestamp'].dt.month\n",
    "trainingData['day'] = trainingData['timestamp'].dt.day\n",
    "trainingData['hour'] = trainingData['timestamp'].dt.hour\n",
    "\n",
    "testingData['year'] = testingData['timestamp'].dt.year\n",
    "testingData['month'] = testingData['timestamp'].dt.month\n",
    "testingData['day'] = testingData['timestamp'].dt.day\n",
    "testingData['hour'] = testingData['timestamp'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0163ebf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots for each timestamp component\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Year Distribution\n",
    "plt.subplot(2, 2, 1)\n",
    "trainingData[trainingData['label'] == 'fake']['year'].value_counts().sort_index().plot(kind='bar', color='lightcoral', alpha=0.8, label='Fake')\n",
    "trainingData[trainingData['label'] == 'real']['year'].value_counts().sort_index().plot(kind='bar', color='skyblue', alpha=0.8, label='Real')\n",
    "plt.title('Year Distribution')\n",
    "plt.legend()\n",
    "\n",
    "# Month Distribution\n",
    "plt.subplot(2, 2, 2)\n",
    "trainingData[trainingData['label'] == 'fake']['month'].value_counts().sort_index().plot(kind='bar', color='lightcoral', alpha=0.8, label='Fake')\n",
    "trainingData[trainingData['label'] == 'real']['month'].value_counts().sort_index().plot(kind='bar', color='skyblue', alpha=0.8, label='Real')\n",
    "plt.title('Month Distribution')\n",
    "plt.legend()\n",
    "\n",
    "# Day Distribution\n",
    "plt.subplot(2, 2, 3)\n",
    "trainingData[trainingData['label'] == 'fake']['day'].value_counts().sort_index().plot(kind='bar', color='lightcoral', alpha=0.8, label='Fake')\n",
    "trainingData[trainingData['label'] == 'real']['day'].value_counts().sort_index().plot(kind='bar', color='skyblue', alpha=0.8, label='Real')\n",
    "plt.title('Day Distribution')\n",
    "plt.legend()\n",
    "\n",
    "# Hour Distribution\n",
    "plt.subplot(2, 2, 4)\n",
    "trainingData[trainingData['label'] == 'fake']['hour'].value_counts().sort_index().plot(kind='bar', color='lightcoral', alpha=0.8, label='Fake')\n",
    "trainingData[trainingData['label'] == 'real']['hour'].value_counts().sort_index().plot(kind='bar', color='skyblue', alpha=0.8, label='Real')\n",
    "plt.title('Hour Distribution')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('figures/TimestampDistributionFakeVsReal.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a99845",
   "metadata": {},
   "source": [
    "## Text content variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d60a6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text\n",
    "fakeCleaned = ' '.join(trainingData[trainingData['label'] == 'fake']['tweetTextClean'].apply(cleanText))\n",
    "realCleaned = ' '.join(trainingData[trainingData['label'] == 'real']['tweetTextClean'].apply(cleanText))\n",
    "\n",
    "# Generate word clouds\n",
    "wordcloudFake = WordCloud(width=800, height=400, background_color='white').generate(fakeCleaned)\n",
    "wordcloudReal = WordCloud(width=800, height=400, background_color='white').generate(realCleaned)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(wordcloudFake, interpolation='bilinear')\n",
    "plt.title('Word Cloud for Fake Tweets')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(wordcloudReal, interpolation='bilinear')\n",
    "plt.title('Word Cloud for Real Tweets')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.savefig('figures/WordClouds.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f236566",
   "metadata": {},
   "source": [
    "## URL Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1133a6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column showing presence of URL\n",
    "urlPattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+])+')\n",
    "trainingData['containsProperUrl'] = trainingData['tweetText'].apply(lambda x : 1 if urlPattern.search(x.replace('\\\\', '')) else 0)\n",
    "testingData['containsProperUrl'] = testingData['tweetText'].apply(lambda x : 1 if urlPattern.search(x.replace('\\\\', '')) else 0)\n",
    "\n",
    "urlCounts = trainingData['containsProperUrl'].value_counts()\n",
    "print(\"Tweet Counts with/without URLs:\")\n",
    "print(\"Contains URL: \", urlCounts.get(1, 0))\n",
    "print(\"Does not contain URL: \", urlCounts.get(0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07144767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample of tweets with inproper URLs\n",
    "tweetsWithoutUrl = trainingData[trainingData['containsProperUrl'] == 0].head(5)\n",
    "\n",
    "for index, row in tweetsWithoutUrl.iterrows():\n",
    "    print(f\"Tweet ID: {row['tweetId']}\")\n",
    "    print(f\"Username: {row['username']}\")\n",
    "    print(f\"Timestamp: {row['timestamp']}\")\n",
    "    print(f\"Tweet Text: {row['tweetText']}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8d02f2",
   "metadata": {},
   "source": [
    "## UserID Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0317775",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetCountsByUser = trainingData['userId'].value_counts()\n",
    "\n",
    "# Categorize users based on tweet counts\n",
    "userBins = pd.cut(tweetCountsByUser, bins=[0, 1, 2, 3, float('inf')], labels=['1 tweet', '2 tweets', '3 tweets', '3+ tweets'])\n",
    "\n",
    "# Count users in each category\n",
    "userCountsInBins = userBins.value_counts()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "userCountsInBins.sort_index().plot(kind='bar', color='skyblue')\n",
    "plt.title('User Distribution by Tweet Counts (Training Data)')\n",
    "plt.xlabel('Number of Tweets')\n",
    "plt.ylabel('Number of Users')\n",
    "\n",
    "plt.savefig('figures/UserDistribution.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04aa069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter users with 4 or more tweets\n",
    "filteredUsers = tweetCountsByUser[tweetCountsByUser >= 4]\n",
    "\n",
    "# Calculate the percentage of total users\n",
    "percentageFilteredUsers = (len(filteredUsers) / len(tweetCountsByUser)) * 100\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "filteredUsers.value_counts().sort_index().plot(kind='bar', color='skyblue')\n",
    "plt.title('Distribution of Users with 4+ Tweets (Training Data)')\n",
    "plt.xlabel('Number of Tweets')\n",
    "plt.ylabel('Number of Users')\n",
    "plt.text(0.4, 0.9, f'{percentageFilteredUsers:.1f}% of Total Users', transform=plt.gca().transAxes, fontsize=10, color='red')\n",
    "\n",
    "plt.savefig('figures/UserDistribution4Plus.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfd1408",
   "metadata": {},
   "source": [
    "## ImageID Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d65e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData['imageId_short'] = trainingData['imageId(s)'].str[:5]\n",
    "testingData['imageId_short'] = testingData['imageId(s)'].str[:5]\n",
    "\n",
    "# Map humor label to fake\n",
    "trainingData['label'] = trainingData['label'].replace('humor', 'fake')\n",
    "testingData['label'] = testingData['label'].replace('humor', 'fake')\n",
    "\n",
    "# Count real and fake\n",
    "imageCounts = trainingData.groupby(['imageId_short', 'label']).size().unstack(fill_value=0)\n",
    "imageCountsTesting = testingData.groupby(['imageId_short', 'label']).size().unstack(fill_value=0)\n",
    "\n",
    "# Calculate total tweet count for each imageId\n",
    "imageCounts['total'] = imageCounts.sum(axis=1)\n",
    "imageCountsTesting['total'] = imageCountsTesting.sum(axis=1)\n",
    "\n",
    "# Sort\n",
    "imageCountsSorted = imageCounts.sort_values(by='total', ascending=False)\n",
    "imageCountsSortedTesting = imageCountsTesting.sort_values(by='total', ascending=False)\n",
    "\n",
    "# Plot the counts testing\n",
    "imageCountsSortedTesting.drop('total', axis=1).plot(kind='bar', stacked=True, figsize=(12, 6))\n",
    "plt.title('Tweet Counts by ImageId and Label (Testing Data)')\n",
    "plt.xlabel('ImageId')\n",
    "plt.ylabel('Tweet Count')\n",
    "plt.legend(title='Label', loc='upper right')\n",
    "\n",
    "plt.savefig('figures/CountsImageIdTesting.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Plot the counts training\n",
    "imageCountsSorted.drop('total', axis=1).plot(kind='bar', stacked=True, figsize=(12, 6))\n",
    "plt.title('Tweet Counts by ImageId and Label (Training Data)')\n",
    "plt.xlabel('ImageId')\n",
    "plt.ylabel('Tweet Count')\n",
    "plt.legend(title='Label', loc='upper right')\n",
    "\n",
    "plt.savefig('figures/CountsImageId.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Exclude the first row\n",
    "imageCountsSortedNoFirst = imageCountsSorted.iloc[1:]\n",
    "\n",
    "# Plot the counts training minus first\n",
    "imageCountsSortedNoFirst.drop('total', axis=1).plot(kind='bar', stacked=True, figsize=(12, 6))\n",
    "plt.title('Tweet Counts by ImageId and Label (Excluding Hurricane Sandy)')\n",
    "plt.xlabel('ImageId')\n",
    "plt.ylabel('Tweet Count')\n",
    "plt.legend(title='Label', loc='upper right')\n",
    "\n",
    "plt.savefig('figures/CountsImageIdNoFirst.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f870e9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the unique imageId\n",
    "uniqueImageIds = trainingData['imageId(s)'].nunique()\n",
    "\n",
    "print(\"Number of unique image IDs (Training Data):\", uniqueImageIds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcbd563",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767f6e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a37464",
   "metadata": {},
   "outputs": [],
   "source": [
    "testingData.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f25ee9",
   "metadata": {},
   "source": [
    "## Remove rows with incorrectly parsed data and low quality data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5f1213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove malformed tweets with length greater than 150\n",
    "filteredTrainingData = trainingData[trainingData['tweetLength'] <= 150]\n",
    "\n",
    "# Remove tweets with inproper URL\n",
    "filteredTrainingData = filteredTrainingData[filteredTrainingData['containsProperUrl'] == 1]\n",
    "\n",
    "# Remove tweets with language detection confidence below 0.7\n",
    "filteredTrainingData = filteredTrainingData[filteredTrainingData['confidence'] > 0.7]\n",
    "\n",
    "# Remove tweets with failed language detection\n",
    "filteredTrainingData = filteredTrainingData[filteredTrainingData['language'] != 'err']\n",
    "\n",
    "# No filtering for testingData\n",
    "filteredTestingData = testingData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c991119",
   "metadata": {},
   "source": [
    "## Translate text (clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c86643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a CSV file\n",
    "trainingFileName = \"dataset/translated_training_data.csv\"\n",
    "testingFileName = \"dataset/translated_testing_data.csv\"\n",
    "\n",
    "progress = None\n",
    "\n",
    "def englishTranslate(row):\n",
    "    text = row['tweetTextClean']\n",
    "    language = row['language']\n",
    "    \n",
    "    # Translate to English only if the language is not English\n",
    "    if language != 'en':\n",
    "        try:\n",
    "            translated = GoogleTranslator(source=language, target='en').translate(text)\n",
    "        except Exception as e:\n",
    "            try:\n",
    "                # Try translating with google language detection\n",
    "                translated = GoogleTranslator(source='auto', target='en').translate(text)\n",
    "            except Exception as e:\n",
    "                translated = None\n",
    "    else:\n",
    "        translated = text  # Keep text if in English\n",
    "    \n",
    "    # Update progress bar\n",
    "    progress.update(1)\n",
    "\n",
    "    return translated\n",
    "\n",
    "def runTranslation(dataset, name, progressType):\n",
    "    global progress \n",
    "    \n",
    "    tweetTextClean = dataset['tweetTextClean']\n",
    "    languages = dataset['language']\n",
    "\n",
    "    # Initialize progress bar\n",
    "    iterations = len(tweetTextClean)\n",
    "    progress = tqdm(total=iterations, desc=progressType + \" Progress\", position=0, leave=True)\n",
    "    \n",
    "    # Apply the translation function\n",
    "    dataset['translatedText'] = dataset.apply(englishTranslate, axis=1)\n",
    "\n",
    "    # Remove entries with translation error\n",
    "    dataset = dataset.dropna(subset=['translatedText'])\n",
    "\n",
    "    # Close progress\n",
    "    progress.close()\n",
    "\n",
    "    # Save the result file\n",
    "    dataset.to_csv(name, index=False)\n",
    "\n",
    "# Check if the training file exists before running translation\n",
    "if not os.path.exists(trainingFileName):\n",
    "    runTranslation(filteredTrainingData, trainingFileName, \"Training\")\n",
    "    \n",
    "# Check if the testing file exists before running translation\n",
    "if not os.path.exists(testingFileName):\n",
    "    runTranslation(filteredTestingData, testingFileName, \"Testing\")\n",
    "\n",
    "translatedTrainingData = pd.read_csv(trainingFileName)\n",
    "translatedTestingData = pd.read_csv(testingFileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d749f40",
   "metadata": {},
   "source": [
    "## Translate text (original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8925c537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "trainingFileName = \"dataset/original_translated_training_data.csv\"\n",
    "testingFileName = \"dataset/original_translated_testing_data.csv\"\n",
    "\n",
    "progress = None\n",
    "\n",
    "def englishTranslate(row):\n",
    "    text = row['tweetText']\n",
    "    language = row['language']\n",
    "    \n",
    "    # Translate to English only if the language is not English\n",
    "    if language != 'en':\n",
    "        try:\n",
    "            translated = GoogleTranslator(source=language, target='en').translate(text)\n",
    "        except Exception as e:\n",
    "            try:\n",
    "                # Try translating with google language detection\n",
    "                translated = GoogleTranslator(source='auto', target='en').translate(text)\n",
    "            except Exception as e:\n",
    "                translated = None\n",
    "    else:\n",
    "        translated = text  # Keep text if in English\n",
    "    \n",
    "    # Update progress bar\n",
    "    progress.update(1)\n",
    "\n",
    "    return translated\n",
    "\n",
    "def runTranslation(dataset, name, progressType):\n",
    "    global progress \n",
    "    \n",
    "    tweetText = dataset['tweetText']\n",
    "    languages = dataset['language']\n",
    "\n",
    "    # Initialize progress bar\n",
    "    iterations = len(tweetText)\n",
    "    progress = tqdm(total=iterations, desc=progressType + \" Progress\", position=0, leave=True)\n",
    "    \n",
    "    # Apply the translation function\n",
    "    dataset['originalTranslatedText'] = dataset.apply(englishTranslate, axis=1)\n",
    "\n",
    "    # Remove entries with translation error\n",
    "    dataset = dataset.dropna(subset=['originalTranslatedText'])\n",
    "\n",
    "    # Close progress bar\n",
    "    progress.close()\n",
    "\n",
    "    # Save file after translation\n",
    "    dataset.to_csv(name, index=False)\n",
    "\n",
    "# Check if the training file exists before running translation\n",
    "if not os.path.exists(trainingFileName):\n",
    "    runTranslation(filteredTrainingData, trainingFileName, \"Training\")\n",
    "    \n",
    "# Check if the testing file exists before running translation\n",
    "if not os.path.exists(testingFileName):\n",
    "    runTranslation(filteredTestingData, testingFileName, \"Testing\")\n",
    "\n",
    "translatedTrainingData = pd.read_csv(trainingFileName)\n",
    "translatedTestingData = pd.read_csv(testingFileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7197f4e0",
   "metadata": {},
   "source": [
    "## Feature engineering - symbol counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e973f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count features for each row\n",
    "def countInText(text):\n",
    "    # Count hashtags\n",
    "    hashtags = len(re.findall(r'#\\w+', text))\n",
    "    \n",
    "    # Find all occurrences of emojis using demoji\n",
    "    emojisList = demoji.findall_list(text)\n",
    "    \n",
    "    # Count the total number of occurrences of emojis\n",
    "    emojisCount = len(emojisList)\n",
    "    \n",
    "    # Count mentions\n",
    "    mentions = len(re.findall(r'@\\w+', text))\n",
    "    \n",
    "    # Count exclamation marks\n",
    "    exclamationMarks = len(re.findall(r'!', text))\n",
    "    \n",
    "    # Count question marks\n",
    "    questionMarks = len(re.findall(r'\\?', text))\n",
    "    \n",
    "    # Check if it is a retweet\n",
    "    retweet = 0\n",
    "    if (len(re.findall(r'RT', text)) > 0):\n",
    "        retweet = 1\n",
    "    \n",
    "    return pd.Series({\n",
    "        'hashtagsCount': hashtags,\n",
    "        'emojisCount': emojisCount,\n",
    "        'mentionsCount': mentions,\n",
    "        'exclamationMarksCount': exclamationMarks,\n",
    "        'questionMarksCount': questionMarks,\n",
    "        'retweet': retweet\n",
    "    })\n",
    "\n",
    "tqdm.pandas(desc=\"Processing Training\", position=0, leave=True)\n",
    "featureCountsTraining = translatedTrainingData['tweetText'].progress_apply(countInText)\n",
    "\n",
    "tqdm.pandas(desc=\"Processing Testing\", position=0, leave=True)\n",
    "featureCountsTesting = translatedTestingData['tweetText'].progress_apply(countInText)\n",
    "\n",
    "# Concatenate the feature counts\n",
    "translatedTrainingData = pd.concat([translatedTrainingData, featureCountsTraining], axis=1)\n",
    "translatedTestingData = pd.concat([translatedTestingData, featureCountsTesting], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3c2b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample of 5 tweets with emoji counts\n",
    "tweetsEmojis = translatedTrainingData[translatedTrainingData['emojisCount'] > 0]\n",
    "sampleTweets = tweetsEmojis.sample(5)\n",
    "\n",
    "for index, row in sampleTweets.iterrows():\n",
    "    print(f\"Tweet: {row['tweetText']}\")\n",
    "    print(f\"Emoji Count: {row['emojisCount']}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a5b71a",
   "metadata": {},
   "source": [
    "## Feature engineering - sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f21f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform sentiment analysis\n",
    "def sentimentAnalysis(text):\n",
    "    analysis = TextBlob(text)\n",
    "    \n",
    "    # Ensure sentimentPolarity is only positive\n",
    "    sentiment_polarity = analysis.sentiment.polarity + 1 if analysis.sentiment.polarity < 0 else analysis.sentiment.polarity\n",
    "    \n",
    "    # Return the sentiment polarity (0 to 2) and subjectivity (0 to 1)\n",
    "    return pd.Series({\n",
    "        'sentimentPolarity': analysis.sentiment.polarity,\n",
    "        'sentimentSubjectivity': analysis.sentiment.subjectivity\n",
    "    })\n",
    "\n",
    "tqdm.pandas(desc=\"Sentiment Analysis Training\")\n",
    "sentimentAnalysisTraining = translatedTrainingData['tweetTextClean'].progress_apply(sentimentAnalysis)\n",
    "\n",
    "tqdm.pandas(desc=\"Sentiment Analysis Testing\")\n",
    "sentimentAnalysisTesting = translatedTestingData['tweetTextClean'].progress_apply(sentimentAnalysis)\n",
    "\n",
    "# Concatenate the sentiment analysis result with the original DataFrame\n",
    "translatedTrainingData = pd.concat([translatedTrainingData, sentimentAnalysisTraining], axis=1)\n",
    "translatedTestingData = pd.concat([translatedTestingData, sentimentAnalysisTesting], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cb92b2",
   "metadata": {},
   "source": [
    "## Select relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c608ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "selectColumns = ['label','translatedText', 'originalTranslatedText', 'tweetText', 'retweet', 'tweetLength', 'year', 'month', 'day', 'hour', 'hashtagsCount', 'emojisCount', 'mentionsCount', 'exclamationMarksCount', 'questionMarksCount', 'sentimentPolarity', 'sentimentSubjectivity']\n",
    "processedTrainingData = translatedTrainingData[selectColumns]\n",
    "processedTestingData = translatedTestingData[selectColumns]\n",
    "\n",
    "processedTrainingFile = \"dataset/processed_training_data.csv\"\n",
    "processedTestingFile = \"dataset/processed_testing_data.csv\"\n",
    "\n",
    "if not os.path.exists(processedTrainingFile):\n",
    "    processedTrainingData.to_csv(processedTrainingFile, index=False)\n",
    "    \n",
    "if not os.path.exists(processedTestingFile):\n",
    "    processedTestingData.to_csv(processedTestingFile, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0362976",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53da4d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training\")\n",
    "print(\"Real count:\", processedTrainingData['label'].isin(['real']).sum(axis=0))\n",
    "print(\"Fake count:\", processedTrainingData['label'].isin(['fake', 'humor']).sum(axis=0))\n",
    "print(\"Total count:\", len(processedTrainingData))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Testing\")\n",
    "print(\"Real count:\", processedTestingData['label'].isin(['real']).sum(axis=0))\n",
    "print(\"Fake count:\", processedTestingData['label'].isin(['fake', 'humor']).sum(axis=0))\n",
    "print(\"Total count:\", len(processedTestingData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34efa5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "processedTrainingData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ec87d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "processedTestingData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b84021d",
   "metadata": {},
   "source": [
    "## Comparison pipeline for classifiers with different vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d227bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizers https://neptune.ai/blog/vectorization-techniques-in-nlp-guide\n",
    "\n",
    "# Get text feature\n",
    "XText = processedTrainingData['tweetText']\n",
    "XTextTest = processedTestingData['tweetText']\n",
    "\n",
    "# Labels\n",
    "YTrain = processedTrainingData['label']\n",
    "YTest = processedTestingData['label']\n",
    "\n",
    "# Convert labels to numerical values\n",
    "YTrain = YTrain.map({'fake': 0, 'real': 1})\n",
    "YTest = YTest.map({'fake': 0, 'real': 1})\n",
    "\n",
    "# Calculate class weights\n",
    "classWeights = compute_class_weight('balanced', classes=[0, 1], y=YTrain)\n",
    "\n",
    "# Set up classifiers\n",
    "classifiers = {\n",
    "    'LinearSVC': LinearSVC(class_weight='balanced', C=1.0, dual=True),\n",
    "    'MultinomialNB': MultinomialNB(alpha=0.01, class_prior=classWeights),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=100, class_weight='balanced'),\n",
    "    'LogisticRegression': LogisticRegression(class_weight='balanced', C=1.0, solver='liblinear'),\n",
    "}\n",
    "\n",
    "# Set up vectorizers\n",
    "vectorizers = {\n",
    "    'CountVectorizer': CountVectorizer(max_features=1000),\n",
    "    'TfidfVectorizer': TfidfVectorizer(max_features=1000),\n",
    "    'HashingVectorizer': HashingVectorizer(n_features=1000, alternate_sign=False)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for classifierName, classifier in classifiers.items():\n",
    "    for methodName, vectorizer in vectorizers.items():\n",
    "        # Build pipeline\n",
    "        pipeline = Pipeline([\n",
    "            ('vectorizer', vectorizer),\n",
    "            ('classifier', classifier),\n",
    "        ])\n",
    "\n",
    "        # Train classifier with progress\n",
    "        with tqdm(total=100, desc=f'Training {classifierName}_{methodName}', position=0, leave=True) as pbar:\n",
    "            pipeline.fit(XText, YTrain)\n",
    "            pbar.update(100)\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        YPred = pipeline.predict(XTextTest)\n",
    "        \n",
    "        # Evaluate the performance\n",
    "        f1 = f1_score(YTest, YPred)\n",
    "        confusion = confusion_matrix(YTest, YPred)\n",
    "        classification = classification_report(YTest, YPred)\n",
    "\n",
    "        results[f'{classifierName}_{methodName}'] = {\n",
    "            'f1_score': f1,\n",
    "            'confusion_matrix': confusion,\n",
    "            'classification_report': classification\n",
    "        }\n",
    "        \n",
    "# Extract F1 score values from results\n",
    "f1Values = np.zeros((len(classifiers), len(vectorizers)))\n",
    "\n",
    "for i, classifierName in enumerate(classifiers.keys()):\n",
    "    for j, methodName in enumerate(vectorizers.keys()):\n",
    "        key = f'{classifierName}_{methodName}'\n",
    "        f1Values[i, j] = results[key]['f1_score']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "width = 0.2\n",
    "positions = np.arange(len(vectorizers))\n",
    "\n",
    "for i, classifierName in enumerate(classifiers.keys()):\n",
    "    bars = ax.bar(positions + i * width, f1Values[i, :], width, label=classifierName)\n",
    "    for bar, value in zip(bars, f1Values[i, :]):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2, height, f'{value:.2f}', ha='center', va='bottom')\n",
    "\n",
    "# Set axis labels and legend\n",
    "ax.set_xticks(positions + (len(classifiers) - 1) * width / 2)\n",
    "ax.set_xticklabels(vectorizers.keys())\n",
    "ax.set_xlabel('Vectorization Method')\n",
    "ax.set_ylabel('F1 Score')\n",
    "\n",
    "plt.title('F1 Score of Classifiers with Different Vectorization Methods')\n",
    "plt.legend(title='Classifier')\n",
    "\n",
    "plt.savefig('figures/ComparisonPipeline.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13efcc9",
   "metadata": {},
   "source": [
    "## LinearSVC with optimized Vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7292351f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training and validation\n",
    "xTrain, xVal, yTrain, yVal = train_test_split(\n",
    "    processedTrainingData['tweetText'], \n",
    "    processedTrainingData['label'], \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Testing\n",
    "xTextTest = processedTestingData['tweetText']\n",
    "yTest = processedTestingData['label']\n",
    "\n",
    "# Convert labels to numerical values\n",
    "yTrain = yTrain.map({'fake': 0, 'real': 1})\n",
    "yVal = yVal.map({'fake': 0, 'real': 1})\n",
    "yTest = yTest.map({'fake': 0, 'real': 1})\n",
    "\n",
    "# Calculate class weights\n",
    "classWeights = compute_class_weight('balanced', classes=[0, 1], y=yTrain)\n",
    "\n",
    "# Set up vectorizers\n",
    "vectorizers = {\n",
    "    'CountVectorizer': CountVectorizer(max_features=4000, stop_words='english'),\n",
    "    'TfidfVectorizer': TfidfVectorizer(stop_words='english'),\n",
    "}\n",
    "\n",
    "# Set up LinearSVC classifier\n",
    "linearSvcClassifier = LinearSVC(class_weight='balanced', C=1.0, dual=True)\n",
    "\n",
    "# Results dictionary\n",
    "results = {}\n",
    "\n",
    "for methodName, vectorizer in vectorizers.items():\n",
    "    # Build pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('vectorizer', vectorizer),\n",
    "        ('classifier', linearSvcClassifier),\n",
    "    ])\n",
    "\n",
    "    # Train classifier\n",
    "    pipeline.fit(xTrain, yTrain)\n",
    "\n",
    "    # Make predictions on the validation set\n",
    "    yValPred = pipeline.predict(xVal)\n",
    "\n",
    "    # Evaluate the performance on the validation set\n",
    "    f1ScoreVal = f1_score(yVal, yValPred, average='weighted')\n",
    "    confusionMatVal = confusion_matrix(yVal, yValPred)\n",
    "    classificationRepVal = classification_report(yVal, yValPred)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    yTestPred = pipeline.predict(xTextTest)\n",
    "\n",
    "    # Evaluate the performance on the test set\n",
    "    f1ScoreTest = f1_score(yTest, yTestPred, average='weighted')\n",
    "    confusionMatTest = confusion_matrix(yTest, yTestPred)\n",
    "    classificationRepTest = classification_report(yTest, yTestPred)\n",
    "\n",
    "    results[f'LinearSVC_{methodName}'] = {\n",
    "        'f1ScoreVal': f1ScoreVal,\n",
    "        'confusionMatrixVal': confusionMatVal,\n",
    "        'classificationReportVal': classificationRepVal,\n",
    "        'f1ScoreTest': f1ScoreTest,\n",
    "        'confusionMatrixTest': confusionMatTest,\n",
    "        'classificationReportTest': classificationRepTest,\n",
    "    }\n",
    "\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}:\\nValidation F1 Score: {value['f1ScoreVal']}\\n{value['classificationReportVal']}\\n\")\n",
    "    print(f\"Test F1 Score: {value['f1ScoreTest']}\\n{value['classificationReportTest']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29a35a4",
   "metadata": {},
   "source": [
    "## LinearSVC with additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c533fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numeric features\n",
    "numericFeatures = ['retweet', 'hashtagsCount',\n",
    "                     'emojisCount', 'mentionsCount', 'exclamationMarksCount', 'questionMarksCount',\n",
    "                     'sentimentPolarity', 'sentimentSubjectivity']\n",
    "\n",
    "# Split training and validation\n",
    "xTrain, xVal, yTrain, yVal = train_test_split(\n",
    "    processedTrainingData[['tweetText'] + numericFeatures], \n",
    "    processedTrainingData['label'], \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Testing\n",
    "xTextTest = processedTestingData[['tweetText'] + numericFeatures]\n",
    "yTest = processedTestingData['label']\n",
    "\n",
    "# Convert labels to numerical values\n",
    "yTrain = yTrain.map({'fake': 0, 'real': 1})\n",
    "yVal = yVal.map({'fake': 0, 'real': 1})\n",
    "yTest = yTest.map({'fake': 0, 'real': 1})\n",
    "\n",
    "# Calculate class weights\n",
    "classWeights = compute_class_weight('balanced', classes=[0, 1], y=yTrain)\n",
    "\n",
    "# Set up vectorizers\n",
    "vectorizers = {\n",
    "    'countVectorizer': CountVectorizer(max_features=4000, stop_words='english'),\n",
    "    'tfidfVectorizer': TfidfVectorizer(max_features=10000, stop_words='english'),\n",
    "}\n",
    "\n",
    "# Set up LinearSVC classifier\n",
    "linearSvcClassifier = LinearSVC(class_weight='balanced', C=1.0, dual=True, max_iter=100000)\n",
    "\n",
    "# Transformer for selecting numeric features\n",
    "class ColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, x):\n",
    "        return x[self.columns]\n",
    "\n",
    "# Preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', vectorizers['countVectorizer'], 'tweetText'),\n",
    "        ('numeric', StandardScaler(), numericFeatures),\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Pipeline setup\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', linearSvcClassifier),\n",
    "])\n",
    "\n",
    "# Cross-validation\n",
    "cvScores = cross_val_score(pipeline, xTrain, yTrain, cv=5, scoring='f1')\n",
    "\n",
    "# Train classifier on the entire training set\n",
    "pipeline.fit(xTrain, yTrain)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "yValPred = pipeline.predict(xVal)\n",
    "\n",
    "# Evaluate the performance on the validation set\n",
    "f1Val = f1_score(yVal, yValPred)\n",
    "confusionMatVal = confusion_matrix(yVal, yValPred)\n",
    "classificationRepVal = classification_report(yVal, yValPred)\n",
    "\n",
    "# Make predictions on the test set\n",
    "yTestPred = pipeline.predict(xTextTest)\n",
    "\n",
    "# Evaluate the performance on the test set\n",
    "f1Test = f1_score(yTest, yTestPred)\n",
    "confusionMatTest = confusion_matrix(yTest, yTestPred)\n",
    "classificationRepTest = classification_report(yTest, yTestPred)\n",
    "\n",
    "print(f\"Cross-Validation Scores: {cvScores}\")\n",
    "print(f\"\\nValidation F1 Score: {f1Val}\\n{classificationRepVal}\\n\")\n",
    "print(f\"Test F1 Score: {f1Test}\\n{classificationRepTest}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe5530d",
   "metadata": {},
   "source": [
    "## Naive Bayes with optimized TD-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fa4222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training and validation\n",
    "xTrain, xVal, yTrain, yVal = train_test_split(\n",
    "    processedTrainingData['tweetText'], \n",
    "    processedTrainingData['label'], \n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Testing\n",
    "xTextTest = processedTestingData['tweetText']\n",
    "yTest = processedTestingData['label']\n",
    "\n",
    "# Convert labels to numerical values\n",
    "yTrain = yTrain.map({'fake': 0, 'real': 1})\n",
    "yVal = yVal.map({'fake': 0, 'real': 1})\n",
    "yTest = yTest.map({'fake': 0, 'real': 1})\n",
    "\n",
    "# Calculate class weights\n",
    "classWeights = compute_class_weight('balanced', classes=[0, 1], y=yTrain)\n",
    "\n",
    "# Set up vectorizers\n",
    "vectorizers = {\n",
    "    'tfidfVectorizer': TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "}\n",
    "\n",
    "# Set up Naive Bayes\n",
    "naiveBayesClassifier = MultinomialNB(alpha=0.1, class_prior=classWeights)\n",
    "\n",
    "# Results dictionary\n",
    "results = {}\n",
    "\n",
    "for methodName, vectorizer in vectorizers.items():\n",
    "    # Build pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('vectorizer', vectorizer),\n",
    "        ('classifier', naiveBayesClassifier),\n",
    "    ])\n",
    "\n",
    "    # Train classifier\n",
    "    pipeline.fit(xTrain, yTrain)\n",
    "\n",
    "    # Make predictions on the validation set\n",
    "    yValPred = pipeline.predict(xVal)\n",
    "\n",
    "    # Evaluate the performance on the validation set\n",
    "    f1Val = f1_score(yVal, yValPred)\n",
    "    confusionMatVal = confusion_matrix(yVal, yValPred)\n",
    "    classificationRepVal = classification_report(yVal, yValPred)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    yTestPred = pipeline.predict(xTextTest)\n",
    "\n",
    "    # Evaluate the performance on the test set\n",
    "    f1Test = f1_score(yTest, yTestPred)\n",
    "    confusionMatTest = confusion_matrix(yTest, yTestPred)\n",
    "    classificationRepTest = classification_report(yTest, yTestPred)\n",
    "\n",
    "    results[f'MultinomialNB_{methodName}'] = {\n",
    "        'f1Val': f1Val,\n",
    "        'confusionMatrixVal': confusionMatVal,\n",
    "        'classificationReportVal': classificationRepVal,\n",
    "        'f1Test': f1Test,\n",
    "        'confusionMatrixTest': confusionMatTest,\n",
    "        'classificationReportTest': classificationRepTest,\n",
    "    }\n",
    "\n",
    "# Print results\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}:\\nValidation F1 Score: {value['f1Val']}\\n{value['classificationReportVal']}\\n\")\n",
    "    print(f\"Test F1 Score: {value['f1Test']}\\n{value['classificationReportTest']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4ce88e",
   "metadata": {},
   "source": [
    "## Final Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22aae92",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1scores = {'LinearSVC': 0.8620505992010652, 'MultinomialNB': 0.7346807828039781}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "bars = ax.bar(f1scores.keys(), f1scores.values(), color=['blue', 'green', 'orange'])\n",
    "\n",
    "for bar, value in zip(bars, f1scores.values()):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, height, f'{value:.4f}', ha='center', va='bottom')\n",
    "\n",
    "ax.set_xlabel('Classifiers with Vectorization Methods')\n",
    "ax.set_ylabel('F1 Score')\n",
    "plt.title('F1 Score of Classifiers')\n",
    "\n",
    "plt.savefig('figures/FinalResults.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
