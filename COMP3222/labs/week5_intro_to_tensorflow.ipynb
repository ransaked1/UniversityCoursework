{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PGyBKZWSianF"
   },
   "source": [
    "# COMP3222/6246 Machine Learning Technologies (2023/24)\n",
    "## Lab 4 - Introduction to Tensorflow\n",
    "\n",
    "This lab is an introduction to the Tensorflow library, a powerful tool to run machine learning algorithms in Python. The Tensorflow library is the backbone of the exercises you will find in lab 5 and lab 6. Its advantages include flexibility, parallel execution, and being a general framework for computation. On top of that, it is a good entry to put in your CV!\n",
    "\n",
    "## 1. Installation\n",
    "\n",
    "First of all, we need to import the library in Python. Some Python distributions have it included already, if yours does not, you can sidestep the issue and use [Google's Collaboratory environment](https://colab.research.google.com/). \n",
    "\n",
    "Still, it can be a good exercise to try and install it on your local machine. \n",
    "\n",
    "Note: If you created a virtual environment using conda, you need to activate it first. It is a good practice to work with virtual environments. See Lab0 - Setting up your environment for details on doing this. \n",
    "\n",
    "In Unix system's you can simply install it by:\n",
    "\n",
    "```\n",
    "pip3 install tensorflow\n",
    "```\n",
    "\n",
    "This will install Tensoflow 2, which will be used throughout this module. \n",
    "\n",
    "After the installation, run this short test to make sure everything is working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tB-grTy6ianI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.11.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tensors and Operations\n",
    "TensorFlow's operations are carried out using tensors. A tensor is a multidimensional array and is very similar to a numpy array. It can also hold scaalr values. It is important to understand how to create and manipulate them when working with custom cost functions, custom layers and so on.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2 3]\n",
      " [4 5 6]], shape=(2, 3), dtype=int32)\n",
      "tf.Tensor(42, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# creating a matrix using tensors\n",
    "A = tf.constant([[1,2,3],[4,5,6]])\n",
    "print(A)\n",
    "\n",
    "# creating a scalar using tensors\n",
    "B = tf.constant(42)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `tf.variable` to create tensors that may need to change and update over time. E.g. weights in a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(3, name=\"x\")\n",
    "y = tf.Variable(2, name=\"y\")\n",
    "z = tf.Variable(1, name=\"z\")\n",
    "g = x*y*z+x*x+z\n",
    "\n",
    "print(g.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RbAhOboIianN"
   },
   "source": [
    "The code above creates a simple function of three variables.\n",
    "\n",
    "*Exercise 1.1.* Modify the code above to compute the value of $f(x,y,z) = x^3 + y^2 + yz + 3$ with $x=-2$, $y=5$ and $z=1.2$\n",
    "Hint: you can use `assign()` method to modify the values of `tf.variable`. E.g. to change the value of $x$, use `x.assign()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Linear Regression\n",
    "In Tensorflow, we can easily define operations on whole arrays, matrices and multi-dimensional matrices (aka tensors). In this section, we look at a straightforward implementation of the vanilla linear regression algorithm.\n",
    "\n",
    "We will use California housing dataset for this section. Details of the dataset can be found at: https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset. You can load the dataset using Scikit-Learn's `fetch_california_housing()` function.\n",
    "\n",
    "**Recap of Linear Regression:**\n",
    "Linear regression model prediction can be written concisely in vector form as $\\mathbf{y} = \\mathbf{w^Tx}$, where $\\mathbf{y}$ is the matrix of target predictions, $\\mathbf{x}$ is the matrix of features and and $\\mathbf{w}$ is the matrix of weights.\n",
    "\n",
    "The mean squared error (MSE) for the linear regression model is $MSE = 1/m \\times \\sum_{i=1}^{m} (w^Tx^{(i)} - y^{(i)})^2$\n",
    "\n",
    "The closed form solution of the weights $\\mathbf{w}$ that minimizes the MSE is called the Normal Equation: $\\mathbf{w} = ((\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T) \\mathbf{y}$. \n",
    "\n",
    "In the above equation, $(\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T)$ is also called the pseudo-inverse of $\\mathbf{X}$.\n",
    "\n",
    "The below snippet of code implements the linear regression formulation discussed above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uwkIsy3FianO"
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__MatMul_device_/job:localhost/replica:0/task:0/device:CPU:0}} Matrix size-incompatible: In[0]: [9,9], In[1]: [20640,1] [Op:MatMul]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m y \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant(housing\u001b[38;5;241m.\u001b[39mtarget\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m Xt \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mtranspose(X)\n\u001b[0;32m---> 14\u001b[0m w \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m weights \u001b[38;5;241m=\u001b[39m w\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(weights)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/MLTlab2324/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/MLTlab2324/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:7215\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7214\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 7215\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__MatMul_device_/job:localhost/replica:0/task:0/device:CPU:0}} Matrix size-incompatible: In[0]: [9,9], In[1]: [20640,1] [Op:MatMul]"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# load the dataset\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "housing_features = np.c_[np.ones((m,1)), housing.data]\n",
    "\n",
    "# define the pseudo-inverse equation in tensorflow\n",
    "X = tf.constant(housing_features, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "Xt = tf.transpose(X)\n",
    "\n",
    "w = tf.matmul(tf.linalg.inv(tf.matmul(Xt, X)), y)\n",
    "\n",
    "weights = w.numpy()\n",
    "\n",
    "print(weights)\n",
    "\n",
    "# # run the computation\n",
    "# with tf.Session() as sess:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YUUhdZrpianS"
   },
   "source": [
    "*Exercise 2.1.* Running the above cell gives you an error. Can you figure out what the error is?\n",
    "Hint: The pseudo-inverse equation in the code above is wrong. Fix the error. Refer to the **recap** above.\n",
    "\n",
    "*Exercise 2.2.* Modify the code above to compute some estimates over the training set. Print the training RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step-by-step hints\n",
    "\n",
    "# calculate y_predicted (hint: tf.matmul(X,w))\n",
    "\n",
    "\n",
    "# calculate error (hint: y - y_pred)\n",
    "\n",
    "\n",
    "# calculate RMSE (hint: use tf.square, tf.reduce_mean and tf.sqrt)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradient descent\n",
    "When the number of features and the dataset are large, computing the pseudo-inverse can become computationally expensive. A more efficient approach is gradient descent, which consists in starting from a randomly selected point and slowly creeping toward the solution. Not only this approach is quick, but it generalises well beyond linear methods. In fact, this is the backbone of the many non-linear neural networks and deep learning algorithms that define the current state-of-the-art.\n",
    "\n",
    "Here is an example of how to implement gradient descent in Tensorflow. In this case, the gradients are computed *automagically* by automatic differentiation. This is a quite fascinating computational technique that saves us from computing first-order derivatives with pen and paper. Link to read more about autodiff - https://www.tensorflow.org/guide/autodiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NPz5Nkc1ianU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 MSE = 2344973.5\n",
      "Step 500 MSE = 1.0362584e+16\n",
      "Step 1000 MSE = 4.580701e+25\n",
      "Step 1500 MSE = inf\n",
      "Step 2000 MSE = inf\n",
      "Step 2500 MSE = inf\n",
      "Step 3000 MSE = inf\n",
      "Step 3500 MSE = inf\n",
      "Step 4000 MSE = inf\n",
      "Step 4500 MSE = nan\n",
      "Step 5000 MSE = nan\n",
      "Step 5500 MSE = nan\n",
      "Step 6000 MSE = nan\n",
      "Step 6500 MSE = nan\n",
      "Step 7000 MSE = nan\n",
      "Step 7500 MSE = nan\n",
      "Step 8000 MSE = nan\n",
      "Step 8500 MSE = nan\n",
      "Step 9000 MSE = nan\n",
      "Step 9500 MSE = nan\n"
     ]
    }
   ],
   "source": [
    "n_steps = 10000\n",
    "learn_rate = 0.01\n",
    "\n",
    "X = tf.constant(housing_features, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "w = tf.Variable(tf.random.uniform([n+1,1], -1.0, 1.0), name=\"w\")\n",
    "    \n",
    "for step in range(n_steps):\n",
    "    if step % 500 == 0:\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_hat = tf.matmul(X, w, name=\"y_hat\")\n",
    "            error = y - y_hat\n",
    "            mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "            print(\"Step\", step, \"MSE =\", mse.numpy())\n",
    "            \n",
    "        gradients = tape.gradient(mse, [w])\n",
    "        w.assign(w - learn_rate * gradients[0])\n",
    "        \n",
    "\n",
    "    \n",
    "w_best = w.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RcYiUxtkianY"
   },
   "source": [
    "*Exercise 3.1.* Add comments to the code above. Do you understand the purpose of each line?\n",
    "\n",
    "*Exercise 3.2.* Perform feature scaling on the dataset and run the gradient descent algorithm again. Do you see any difference in the result? What about the number of steps needed to converge to the optimum?\n",
    "\n",
    "*Hint for feature scaling:* Before feeding the training set to your favourite ML algorithm, it is good practice to normalise the input features. This means scaling them so that their values fall more or less in the same range. The scikit library offers two different scaling methods: min-max scaling, and standard scaling.\n",
    "\n",
    "`MinMaxScaler` in the Scikit-learn library transforms features by scaling each feature to a given range; see https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler. \n",
    "\n",
    "`StandardScaler` in the Scikit-learn library transforms features to have zero mean and unit variance; see https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html. \n",
    "\n",
    "\n",
    "The below code demonstrates the use of `MinMaxScaler`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 MSE = nan\n",
      "Step 500 MSE = 5.7178645\n",
      "Step 1000 MSE = 5.5919514\n",
      "Step 1500 MSE = 5.4695787\n",
      "Step 2000 MSE = 5.3506474\n",
      "Step 2500 MSE = 5.2350597\n",
      "Step 3000 MSE = 5.122721\n",
      "Step 3500 MSE = 5.0135427\n",
      "Step 4000 MSE = 4.907431\n",
      "Step 4500 MSE = 4.804302\n",
      "Step 5000 MSE = 4.704071\n",
      "Step 5500 MSE = 4.6066566\n",
      "Step 6000 MSE = 4.5119796\n",
      "Step 6500 MSE = 4.419962\n",
      "Step 7000 MSE = 4.3305287\n",
      "Step 7500 MSE = 4.2436075\n",
      "Step 8000 MSE = 4.159127\n",
      "Step 8500 MSE = 4.077019\n",
      "Step 9000 MSE = 3.9972165\n",
      "Step 9500 MSE = 3.9196532\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(housing_features)\n",
    "scaled_housing_features = scaler.transform(housing_features)\n",
    "\n",
    "n_steps = 10000\n",
    "learn_rate = 0.01\n",
    "\n",
    "scaled_X = tf.constant(scaled_housing_features, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "w = tf.Variable(tf.random.uniform([n+1,1], -1.0, 1.0), name=\"w\")\n",
    "    \n",
    "for step in range(n_steps):\n",
    "    if step % 500 == 0:\n",
    "        print(\"Step\", step, \"MSE =\", mse.numpy())\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_hat = tf.matmul(scaled_X, w, name=\"y_hat\")\n",
    "            error = y - y_hat\n",
    "            mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "    \n",
    "        gradients = tape.gradient(mse, [w])\n",
    "        w.assign(w - learn_rate * gradients[0])\n",
    "         \n",
    "w_best = w.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the above code using `StandardScaler` and observe the difference.\n",
    "\n",
    "*Exercise 3.3.* The gradient descent algorithm is really sensitive to the value of the learning rate. Try changing it by a few orders of magnitude in the above and run the algorithm again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Regression MLPs\n",
    "\n",
    "You can use Multilayer Perceptrons for regression tasks. If you are predicting a single value (e.g. price of a house given some features), you need one output neuron. You can also predict multiple values, in which case, the task is called multivariate regression and you would need one output neuron per output dimension. When building an MLP for regression, you may not choose to use activation functions so that they can output freely any range of values. Or you can choose to use certain activation functions to constrain the range of the output values.\n",
    "\n",
    "Using `tf.keras`, which implements the Keras API with some extra TensorFlow-speific features, we will build an MLP to perform the Regression task. Specifically, we will use Sequential API, which enables a straightforward way to build a neural network in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F_Kxfu44ianZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "122/363 [=========>....................] - ETA: 0s - loss: 3.7457 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-02 10:13:57.406861: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 0s 646us/step - loss: 2.2766 - val_loss: 2.7245\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 462us/step - loss: 0.9135 - val_loss: 1.2719\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 466us/step - loss: 0.7419 - val_loss: 0.7762\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 441us/step - loss: 0.6801 - val_loss: 0.6681\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 462us/step - loss: 0.6415 - val_loss: 0.6085\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 456us/step - loss: 0.6091 - val_loss: 0.5738\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 455us/step - loss: 0.5816 - val_loss: 0.5462\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 457us/step - loss: 0.5572 - val_loss: 0.5214\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 454us/step - loss: 0.5353 - val_loss: 0.5002\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 456us/step - loss: 0.5161 - val_loss: 0.4821\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 457us/step - loss: 0.4996 - val_loss: 0.4660\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 458us/step - loss: 0.4850 - val_loss: 0.4507\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 464us/step - loss: 0.4726 - val_loss: 0.4393\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 451us/step - loss: 0.4617 - val_loss: 0.4316\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 467us/step - loss: 0.4524 - val_loss: 0.4272\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 455us/step - loss: 0.4445 - val_loss: 0.4227\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 453us/step - loss: 0.4378 - val_loss: 0.4257\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 493us/step - loss: 0.4320 - val_loss: 0.4285\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 474us/step - loss: 0.4269 - val_loss: 0.4216\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 467us/step - loss: 0.4228 - val_loss: 0.4332\n"
     ]
    }
   ],
   "source": [
    "# To allow training and testing, we need to split the California housing data into training, validation and test sets\n",
    "# We also use StandardScaler for feature scaling -- try other scaling methods on your own.\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=tf.keras.optimizers.SGD(learning_rate=1e-3))\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "53PPWu2-iane"
   },
   "source": [
    "*Exercise 4.1.* Add comments to the code above. Do you understand the purpose of each line?\n",
    "Hint: The code above uses Sequnetial API to build and train a Regression MLP. `tf.keras.models.Sequential` creates a neural network composed of single stack of layers connected sequentially. `tf.keras.layers.Dense` is used to create a Dense layer that manages its own weight matrix with connection weights between the neurons and their input. You can also add an activation function to the layer. \n",
    "\n",
    "Hint: In `model.compile` try changing the learning rate and run the model again.\n",
    "\n",
    "Hint: The number of epochs are defined in `model.fit`. You can also pass a validation set that will be used to calculate validation accuracy. Notice that the training loss and validation loss are different. \n",
    "\n",
    "**Plotting Learning Curves:** `model.fit` return history object containing training paramters (`history.params`), list of epochs (`history.epoch`) and a dictionary containing loss and other metrics measured at the end of each epoch (`history.history`). You can use pandas DataFrame and `plot()` method to plot learning curves as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(pd\u001b[38;5;241m.\u001b[39mDataFrame(history\u001b[38;5;241m.\u001b[39mhistory))\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mgrid(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(pd.DataFrame(history.history))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that you created a test set in the above code. Using the test set, do the following exercises. \n",
    "\n",
    "*Exercise 4.2.* Calculate the MSE on the test set. \n",
    "Hint: Use `model.evaluate`. Link for more information: https://www.tensorflow.org/api_docs/python/tf/keras/Model#evaluate.\n",
    "\n",
    "*Exercise 4.3.* Using a datapoint from the test set, predict the output. Hint: Use `model.predict`. Link for more information: https://www.tensorflow.org/api_docs/python/tf/keras/Model#predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step-by-step hints\n",
    "\n",
    "#Calculate MSE on the test set using 'model.evaluate(X_test,Y_test)''\n",
    "\n",
    "\n",
    "#Pick a datapoint from the test set --> X_new = X_test[:1]\n",
    "\n",
    "\n",
    "#Calculate y_pred using 'model.predict(X_new)'\n",
    "\n",
    "\n",
    "#print y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This week's Summary\n",
    "We have looked at some basic functionality of Tensorflow. TensorFlow offers a wide range of features. The most important is `tf.keras`, which you will be using frequently in this module to carry out building and training networks. However, there are other key features for loading and preprocessing data, image processing and signal processing. \n",
    "\n",
    "In the coming weeks, we will explore more funtionality of Tensorflow while building and training more complex networks. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lab4.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
