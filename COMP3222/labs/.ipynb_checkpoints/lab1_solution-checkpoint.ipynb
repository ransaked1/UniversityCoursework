{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dOguF95OyAOs"
   },
   "source": [
    "# COMP3222/6246 Machine Learning Technologies (2023/24)\n",
    "# Lab 1 – Basics of Machine Learning project\n",
    "\n",
    "In this week, we will have an overview of how a practical Machine Learning project works. We aim to familiarise you with the general procedure of doing Machine Learning, while encouraging you to develop your critical thinking by asking you some questions now and then. It also provides an introduction to Scikit-learn, an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection, model evaluation, and many other utilities.\n",
    "\n",
    "In general, a Machine Learning project is not different from a software project, where you might want to go back and forth and tweak something, or roll out the first prototype and improve on it incrementally. Answering the questions will help you understand more, and allow you to come up with an idea for improving the Machine Learning prototype we introduced here.\n",
    "\n",
    "This exercise is based on the one featured in Chapter 2 of the course's text book.  https://southampton.on.worldcat.org/oclc/1347020175. It is simplified in some aspects to fit into two hours of effort, and to fit the structure of the material given in the lectures. We also add a few additional explanations of our own that we believe make things clearer regarding how Scikit works, and some exercises of our own. As an additional activity , you may read the chapter in question and follow along the linked notebook (In Google Colab). There are some bits of it like the Data Pipelines, that we will cover in following weeks.  \n",
    "\n",
    "Note that you will not learn by simply executing this notebook without thinking a bit for yourself :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell is to check we have the right versions of the tools we \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "\n",
    "print(\"Numpy version: {}\".format(np.__version__))\n",
    "print(\"Pandas version: {}\".format(pd.__version__))\n",
    "print(\"Scikit version: {}\".format(sk.__version__))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Checklist\n",
    "\n",
    "The textbook suggest (Appendix A) the following high level checklist to tackle Machine Learning Projects\n",
    "\n",
    "1. Frame the problem and look at the big picture.\n",
    "2. Get the data.\n",
    "3. Explore the data to gain insights.\n",
    "4. Prepare the data to better expose the underlying data patterns to machine learning algorithms.\n",
    "5. Explore many different models and shortlist the best ones.\n",
    "6. Fine-tune your models and combine them into a great solution.\n",
    "7. Present your solution.\n",
    "8. Launch, monitor, and maintain your system\n",
    "\n",
    "In this session we will review steps 1-4 and the start of 5 (implement a single model). Next week we will delve deeper in steps 5) and 6).\n",
    "\n",
    "We will only touch upon step 7) briefly. This is more a concern of Data Visualisation, a module that some of our MScs are taking. It is optional Part IV for those undergrads that proceed to MEng.\n",
    "\n",
    "Step 8) is out of scope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_iIy8cEHyAOs"
   },
   "source": [
    "## Frame the problem and big picture\n",
    "\n",
    "Although one can derive great pleasure from engineering ML models that beat hard benchmarks and win Kaggle challenges, the  reality is that most of you will be either hired to solve a company's problem, or become entrepreneurs to solve a customer's problem.\n",
    "\n",
    "In any case, it is essential you frame the problem that you will be solving in the business need of the company/customer. Otherwise, you may end up with an accurate an efficient model that is absolutely useless for the people paying your salary.\n",
    "You also aim at answering the following technical questions:\n",
    "\n",
    "* How the problem is currently solved? The point here is to get a baseline against which compare your model\n",
    "* What kind of training supervision the model will need: is it a supervised, unsupervised, semi-supervised, self-supervised, or reinforcement learning task? \n",
    "* Is it a classification task, a regression task, or something else? (Maybe not even an ML model!!) \n",
    "* Should you use batch learning or online learning techniques?\n",
    "\n",
    "\n",
    "In this exercise, we imagine we have been hired by a Real State company interested in investing in the State of California, USA. They want to **predict the median housing price in a district** to assess if it's worth to buy property in a given district. Currently, a team of experts do it manually, but a recent evaluation found that they were off by 30%.... prompting management to approve your hiring to improve the situation. They would like to start the experiment using census data, that is openly available and includes the per district median house pricing at a given year, plus additional demographic information such as population, median income and median age of the houses.\n",
    "\n",
    "An how the company would use it? Elsewhere, they have a vey good estimation of the expected changes in districts' demography. For example, when they get word a large company is establishing headquarters in a certain district, they predict the population and median income will increase 10% and 5% respectively in 5 years, then, use your model to predict the median house pricing of this \"future state of the district\" to assess if they should buy some houses now to reap benefits in 5 years. [Note the textbook has a different valid context] \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HUGoY8cDyAOt"
   },
   "source": [
    "## Getting the dataset\n",
    "\n",
    "In this exercise, this step is very easy, as management already pointed out what is the dataset they would like to use. Unfortunately, you are not always that lucky. Data collection is a very important process, several orders of magnitude more laborious than typing Scikit commands while sipping a frappuccino [1] , and strongly dependent on the business scenario.\n",
    "\n",
    "Think about all the effort needed to collect the data of this example: knock every door and get people tell you the number of dwellers and annual income; extract the data on housing age from the councils; scan sale contract pdfs to get houses' prices......\n",
    "\n",
    "Your analysis might reveal that more data is needed, or a different data is needed. Assessing how much collecting that data could cost and the potential increase in accuracy is an important part of framing yourself in the big picture. \n",
    "\n",
    "We will use census data provided in the textbook.\n",
    "\n",
    "(Note: In the textbook, this is the part where they explain Jupyter Notebooks, that we already covered last week.)\n",
    "\n",
    "\n",
    "\n",
    "[1] BTW, remember only water bottles in the lab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RCSZTHAPyAOt"
   },
   "outputs": [],
   "source": [
    "# Jane, the ever efficient Data Engineer of the team, has kindly provided the script below\n",
    "# to download and decompress the data from its current location into a local folder for your peruse in your\n",
    "# local notebook.\n",
    "# Details are out of scope. \n",
    "# If you feel this is cooler than models, you might become a Data Engineer in the future! (they get well paid too)\n",
    "from pathlib import Path\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "def load_housing_data():\n",
    "    tarball_path = Path(\"datasets/housing.tgz\")\n",
    "    if not tarball_path.is_file():\n",
    "        Path(\"datasets\").mkdir(parents=True, exist_ok=True)\n",
    "        url = \"https://github.com/ageron/data/raw/main/housing.tgz\"\n",
    "        urllib.request.urlretrieve(url, tarball_path)\n",
    "        with tarfile.open(tarball_path) as housing_tarball:\n",
    "            housing_tarball.extractall(path=\"datasets\")\n",
    "    return pd.read_csv(Path(\"datasets/housing/housing.csv\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xwXimItuyAO0"
   },
   "source": [
    "After running the above, you should have now a folder \"datasets\" in the same folder where the lab's ipynb file is. Inside that, a \"housing\" folder, and inside that a housing.csv.\n",
    "\n",
    "Moreover, as Jane took this module before deciding to change her career path, she knows about pandas, a library for data analysis and statistics that is very helpful for doing dataset exploration. She used the method read_csv to load housing.csv into a DataFrame, an in-memory structure where is very easy to analyse.    \n",
    "\n",
    "You owe Jane a frappuccino. A big one. With extra syrup.\n",
    "\n",
    "Wile she drinks, read the 10min introduction to Pandas https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html to familiarise with the concepts of Series and DataFrames and what you can do with them. You will notice numpy arrays from last week form the basis of Series and DataFrames, and that many of the operation you learned last week for arrays also apply to Series and DataFrames.\n",
    "\n",
    "If you need to run some of the examples in the guide, create a cell below and have fun.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J5ji2f9XyAO1"
   },
   "source": [
    "## Exploring the dataset\n",
    "\n",
    "Now you have the dataset loaded in memory, it's time to interrogate it. You are asking it:\n",
    "\n",
    "* What type of variables it has (*numerical* or *categorical*?). \n",
    "* How many data points useful for my problem?\n",
    "* Are there missing values? If so, in what variable(s)? how many?\n",
    "* Has any variable been transformed or manipulated before it got to you?    \n",
    "\n",
    "This is where pandas adds value over pure numpy, offering a number of convenient methods to calculate descriptive statistics.  \n",
    "\n",
    "Skim over the DataFrame methods for \n",
    "* underlying data https://pandas.pydata.org/docs/reference/frame.html#attributes-and-underlying-data, and\n",
    "* descriptive stats: https://pandas.pydata.org/docs/reference/frame.html#computations-descriptive-stats\n",
    "\n",
    "and find the right one to implement desired interrogation of the housing variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1mfOjZZSyAO1"
   },
   "outputs": [],
   "source": [
    "# Number of columns and number of rows \n",
    "housing = load_housing_data()\n",
    "\n",
    "print(\"Number of colums {}\".format(len(housing.columns)))\n",
    "print(\"Number of rows {}\".format(housing.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a concise summary of the dataset\n",
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables of dtype object are usually categorical (except if there was an error loading the dataset) \n",
    "# List the different values of the categorical variable in 'housing' and count the occurences\n",
    "\n",
    "housing[\"ocean_proximity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One variable appears to have some null values\n",
    "# Create a dataframe with the rows that have the value of that variable = null  \n",
    "# Hint: read this https://pandas.pydata.org/docs/getting_started/intro_tutorials/03_subset_data.html\n",
    "\n",
    "no_bedrooms = housing[housing['total_bedrooms'].isna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NHLSJWGoyAO4"
   },
   "outputs": [],
   "source": [
    "no_bedrooms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "czt7hWfEyAO7"
   },
   "outputs": [],
   "source": [
    "# generate descriptive statistics of the housing dataset \n",
    "\n",
    "housing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "65JMniM4yAPA"
   },
   "outputs": [],
   "source": [
    "# Another way to get more insight is by plotting histograms\n",
    "# If you don't remember what a histogram is, go here https://chartio.com/learn/charts/histogram-complete-guide/\n",
    "# Use the hist command to for each variable, plot a histogram with 50 bins and a 20x15 size.\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.hist.html\n",
    "\n",
    "housing.hist(bins=50, figsize=(20,15)) \n",
    "# You may try experimenting with different number of bins to discover other patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X5F4OdFYyAPC"
   },
   "source": [
    "Now that we have the numbers and the figures, it's time to answer the questions we set up at the beginning\n",
    "\n",
    "* The type of variables it has (*numerical* or *categorical*?): \n",
    "    6 numerical, 2 geographical, 1 categorical \n",
    "* How many data points useful for my problem?\n",
    "    Initially all 20640, because all contain the target variable median_house_value:   \n",
    "* Are there missing values? If so, in what variable(s)? how many?:\n",
    "    207 data points miss the total_bedrooms variable. If we believe total_bedrooms a useful feature for our model, something will have to be done with those data points.\n",
    "* Has any variable been transformed or manipulated before it got to you? :   \n",
    "   This one is for you ;). Based on the stats and the histograms, what three variables have been manipulated before you? (hint, two of them have the same problem)\n",
    "   \n",
    "  1. median income has very low values, it has been scaled by 10000 and capped at max. 15 (150000) and min 0.5 (5000)\n",
    "  2. housing_median_age has been capped at max 52 years \n",
    "  3. median_house_value has been capped at max 500000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UETCxzhPyAPD"
   },
   "source": [
    "## Defining the problem and a performance measure\n",
    "\n",
    "After our exploration, we are ready to define our problem. For that we need to answer the following questions.\n",
    "\n",
    "1) is it a supervised, unsupervised, semi-supervised, self-supervised, or reinforcement learning task?\n",
    "2) is it a classification task, a regression task, or something else?\n",
    "3) should you use batch learning or online learning techniques? \n",
    "\n",
    "\n",
    "The answers for our case are:\n",
    "\n",
    "1) Supervised, we can train with labeled examples\n",
    "2) Regression, since the model will be asked to predict a value. In particular, multiple regression since multiple features will be used for prediction. It is also univariate prediction as we are only predicting a single value per district.\n",
    "3) Batch learning, as the data is small and does not change rapidly\n",
    "\n",
    "\n",
    "The next  pick a performance measure of our Machine Learning algorithm beforehand. There are a number of performance measures for regression task, but we will just use the Root Mean Square Error (RMSE) for now.\n",
    "\n",
    "$$\\text{RMSE} \\left( \\mathbf{Y} , \\mathbf{\\hat{Y}} \\right) = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n \\left( y_i - \\hat{y_i} \\right)^2 } $$\n",
    "\n",
    "where $\\mathbf{Y}$ and $\\mathbf{\\hat{Y}}$ are an $n$-sized vector of true values and an $n$-sized vector of predicted values which comprises of $y_i$ and $\\hat{y_i}$ respectively for each datapoint $i$. In other words, RMSE is computed from a square root of an average squared error.\n",
    "\n",
    "Another well-known performance measure is the Mean Absolute Error (MAE), which is computed by taking an average of an absolute value of the error.\n",
    "\n",
    "$$\\text{MAE} \\left( \\mathbf{Y} , \\mathbf{\\hat{Y}} \\right) = \\frac{1}{n} \\sum_{i=1}^n | y_i - \\hat{y_i} | $$\n",
    "\n",
    "Go to page 43-45 of the textbook https://southampton.on.worldcat.org/oclc/1347020175 and understand the difference between the two. What rule of thumb can be used to choose the most appropriate? \n",
    "\n",
    "What statistical plot would you use to apply that rule of thumb to the housing dataset? (use external resource if you don't remember) \n",
    "\n",
    "Hint: Check plots available in pandas here https://pandas.pydata.org/docs/reference/frame.html#plotting \n",
    "\n",
    "Hint2: You can Google it if you don't remember.\n",
    "\n",
    "What can be said about\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The rule of thumb is that RMSE is generally preferred except \n",
    "# if there are many outliers\n",
    "\n",
    "# We use a boxplot\n",
    "housing.boxplot(column='median_house_value')\n",
    "\n",
    "# There are no outliers, so we should use RSME, but the max cap creates another issue, that we explain in the next section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4U4r6lDKyAPE"
   },
   "source": [
    "## Data cleaning: missing values and capped values\n",
    "\n",
    "Our exploration concludes that we have one variable with missing values, two variables with capped values (including the target one) and one scaled down variable. The most important issue is the cap on the target variable. Your models may learn that prices never go beyond that limit. You need to check with your client team (the team that will use your system’s output) to see if this is a problem or not. If they tell you that they need precise predictions even beyond $500,000, then you have two options:\n",
    "\n",
    " 1. Collect proper labels for the districts whose labels were capped.\n",
    " 2. Remove those districts from the training set (and also from the test set, since your system should not be evaluated poorly if it predicts values beyond $500,000).\n",
    " \n",
    " The second most important issue is missing data in a variable, the options are, in order of consideration,:\n",
    " \n",
    "  1. Collect proper labels for those districts\n",
    "  2. Decide that you don't use that variable as a feature of your model \n",
    "  3. Replace the null value with a sensible non-null value\n",
    "  4. Discard all data points with null-values  \n",
    "  \n",
    "  \n",
    "  Sadly, we already know option (1) is too expensive in our case, we can't run the census again on those districts. \n",
    "  \n",
    "  To assess option (2) you need a combination of technical and business knowledge. From a technical point of view, you can calculate the correlation between the target variable and the variable you are considering discarding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the corr method to compute the correlation between all pairs of variables in the housing DataFrame\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html\n",
    "# you may need to workaround the categorical variable in the dataset   \n",
    "\n",
    "#The numeric_only argument requires pandas >= 1.5\n",
    "housing.corr(numeric_only=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation is quite low, but that cannot be the only factor, you need business. Jim from Business confirms that despite the fact that there is low correlation total_bedrooms is something they can predict accurately in the future, so they are keen to check if it's an useful variable or not and they'd rather you keep it. They also confirm there is no sensible non-null value to replace the missing data, so you are left with option (4), discard the null data points. \n",
    "\n",
    "Furthermore, they'd also like to have predictions for housing ages greater than 52 and house values greater than 500k, so those data points must go too. Jim is sorry about all the extra work for you. You hang up the Teams call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Q6tCiuByAPE"
   },
   "outputs": [],
   "source": [
    "# Using an appropriate filter, discard data points with null total_bedrooms, housing_median_age >= 52 and median_house_value=500001  \n",
    "\n",
    "fltr_idx = housing['total_bedrooms'].notna() & (housing['housing_median_age'] < 52) & (housing['median_house_value'] < 500001) # retrieve boolean array where each value corresponds to datapoint we want\n",
    "fltr_housing = housing[fltr_idx].reset_index(drop=True) # This bit resets the indexes of the DataFrame\n",
    "fltr_housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MMTzSZk-yAPH"
   },
   "outputs": [],
   "source": [
    "# We then check the descriptive stats again\n",
    "\n",
    "fltr_housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mtFixD4_yAPJ"
   },
   "outputs": [],
   "source": [
    "fltr_housing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OoVsJ_ZByAPL"
   },
   "outputs": [],
   "source": [
    "fltr_housing.hist(bins=50, figsize=(20,15)) \n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HV5SgfC7yAPP"
   },
   "source": [
    "## Dealing with categorical attributes\n",
    "\n",
    "The final data issue we have is the categorical attribute 'ocean_proximity'. Since most Machine Learning algorithms work on numerical vectors and matrices only, we need to transform the categorical attribute to a sensible numerical value that still represent its original meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lpdf3ij-yAPP"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoded_cat, categories = fltr_housing[\"ocean_proximity\"].factorize() # retrieve the attribute encoded as numbers\n",
    "encoded_cat_arr = OneHotEncoder().fit_transform(encoded_cat.reshape(-1,1)).toarray() # transform sparse matrix to NumPy array\n",
    "enc_fltr_housing = fltr_housing.iloc[:,0:9].copy()\n",
    "for i in range(0, len(categories)):\n",
    "    enc_fltr_housing[categories[i]] = encoded_cat_arr[:,i]\n",
    "enc_fltr_housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WqOaun0MyAPT"
   },
   "source": [
    "We have just finished transforming each categorical value to a vector of binary values. As an alternative, we could have only a single numerical attribute that maps to the categories; e.g. 1 for 'NEAR BAY', 2 for '<1H OCEAN', etc. Compared to having a vector of binary values, what are the pros&cons of this approach? Will there be any problem later if we use this approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5iljMgBpyAPT"
   },
   "source": [
    "## Data partitioning: train set and test set\n",
    "\n",
    "A critical part of a Machine Learning project is to separate the available data into a training set and a test set. The most common way of splitting is randomly selecting a sample of 20% as the test set. However, it is important to ensure your split is reproducible, i.e, remains the same for the duration of your project. Otherwise, if you run the notebook the enxt day, you would have scrmabled everything!\n",
    "\n",
    "Use the Scikit-learn's train_test_split method to split the fltr_housing dataset in 80% training and 20% test in a reproducible manner\n",
    " \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "Note: The textbook splits the dataset earlier than us, that is indeed better but it has the caveat that if you do transformations to the training set (e.g. one hot encoding) you have to remember them so you apply them to the test set. We will cover how to do this next week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(enc_fltr_housing, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Primer on Scikit-learn\n",
    "\n",
    "Slides link here: https://sotonac-my.sharepoint.com/:p:/r/personal/ldig1y14_soton_ac_uk/Documents/Education/2023-Term1/COMP3222%20-%20MLTech/Lab%20Materials/Lab-Week2-Appendix-Scikit-learn.pptx?d=w125cd2d5cbf34133afb66a277534e1b3&csf=1&web=1&e=at0QGn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GbVIFbzIyAPh"
   },
   "source": [
    "## Choose and apply Machine Learning algorithm\n",
    "\n",
    "After so much work on preparing our dataset, we are ready to try our Machine Learning algorithm. Whilst there are many algorithms or *models* for regression task, let us apply the basic approach first: the Linear Regression algorithm. In many cases, a simple model such as the Linear Regression works perfectly fine. If the simple model is sufficient, then there is no need to apply complex algorithms which could require the tuning of many hyperparameters, larger number of datapoints, or longer time to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MtqJjV53yAPi"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Make sure you understand what we are doing here\n",
    "# Describe each line with a comment\n",
    "lnr_regressor1 = LinearRegression()\n",
    "lnr_regressor1.fit(train_set.iloc[:, [idx for idx in range(len(train_set.columns)) if idx != 8]], train_set['median_house_value'])\n",
    "prediction = lnr_regressor1.predict(test_set.iloc[:, [idx for idx in range(len(test_set.columns)) if idx != 8]])\n",
    "print('RMSE = ', np.sqrt(mean_squared_error(test_set['median_house_value'], prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LE8tfHRMyAPk"
   },
   "source": [
    "We have just trained a linear regression model based on our training set. Then, we have used it to predict the house price on our test set, and we have computed the RMSE to quantify how good our model is. Clearly, the RMSE we have got is very high. That implies that our Machine Learning algorithm is not performing well enough. Will the RMSE change if we redo everything again? What could have gone wrong? What could be done to improve our accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9tE0OtGDyAPl"
   },
   "source": [
    "## Feature engineering\n",
    "\n",
    "We have just finished our first prototype, but it doesn't seem to work well. As someone say: 'Garbage in, Garbage out'! So, it could be the case that our dataset is not comprised of useful attributes that are going to help our Machine Learning algorithm to learn and predict well. \n",
    "\n",
    "Besides your earlier discussion with Jim about what features should be discarded, it is also possible to experiment with combinations of attributes. Speaking of Jim, he just sent you an email. He has been thinking about it and from a business perspective you don't care much about district's total rooms and total bedrooms, you care about number of rooms per houes, ratio of bedrooms over rooms, and number of people per house. Let's engineer the features and see how much our Machine Learning algorithm will improve. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7NU3x12zyAPl"
   },
   "outputs": [],
   "source": [
    "# Create a copy of the training set and add three columns\n",
    "# with number of rooms per house, ratio of bedrooms over rooms, and number of people per house\n",
    "\n",
    "train_set2 = train_set.copy()\n",
    "train_set2['room_per_house'] = train_set2['total_rooms']/train_set2['households']\n",
    "train_set2['bedroom_per_room'] = train_set2['total_bedrooms']/train_set2['total_rooms']\n",
    "train_set2['pop_per_house'] = train_set2['population']/train_set2['households']\n",
    "train_set2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MfISVHRLyAPn"
   },
   "outputs": [],
   "source": [
    "test_set2 = test_set.copy()\n",
    "test_set2['room_per_house'] = test_set2['total_rooms']/test_set2['households']\n",
    "test_set2['bedroom_per_room'] = test_set2['total_bedrooms']/test_set2['total_rooms']\n",
    "test_set2['pop_per_house'] = test_set2['population']/test_set2['households']\n",
    "test_set2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7KhjZKpwyAPq"
   },
   "outputs": [],
   "source": [
    "# Create a new Linear Regression estimator on the new data frame, fit, predict and evaluate.\n",
    "\n",
    "lnr_regressor2 = LinearRegression()\n",
    "lnr_regressor2.fit(train_set2.iloc[:, [idx for idx in range(len(train_set2.columns)) if idx != 8]], train_set2['median_house_value'])\n",
    "prediction2 = lnr_regressor2.predict(test_set2.iloc[:, [idx for idx in range(len(test_set2.columns)) if idx != 8]])\n",
    "print('RMSE = ', np.sqrt(mean_squared_error(test_set2['median_house_value'], prediction2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dM6NvGJ9yAPs"
   },
   "source": [
    "Whilst the improvement is not that significant, it has shown that *feature engineering* is very useful. There are more techniques than those we have just shown. Could you name some?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oLdMndvsyAPs"
   },
   "source": [
    "## Choose and apply Machine Learning algorithm (again)\n",
    "\n",
    "It is possible that the Linear Regression model is not powerful enough to learn from our dataset. We could try different regression models: say, the Random Forest Regression. With Scikit-Learn, we can try many different algorithms easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NVF9VZO7yAPt"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "frst_regressor1 = RandomForestRegressor()\n",
    "frst_regressor1.fit(train_set.iloc[:, [idx for idx in range(len(train_set.columns)) if idx != 8]], train_set['median_house_value'])\n",
    "prediction3 = frst_regressor1.predict(test_set.iloc[:, [idx for idx in range(len(test_set.columns)) if idx != 8]])\n",
    "print('RMSE = ', np.sqrt(mean_squared_error(test_set['median_house_value'], prediction3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zQy7BD_vyAPw"
   },
   "source": [
    "With the Forest Regression model, we have achieved a good improvement on the non-engineered dataset. Now, we could also try it on the engineered dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KGvh15wQyAPw"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "frst_regressor2 = RandomForestRegressor()\n",
    "frst_regressor2.fit(train_set2.iloc[:, [idx for idx in range(len(train_set2.columns)) if idx != 8]], train_set2['median_house_value'])\n",
    "prediction4 = frst_regressor2.predict(test_set2.iloc[:, [idx for idx in range(len(test_set2.columns)) if idx != 8]])\n",
    "print('RMSE = ', np.sqrt(mean_squared_error(test_set2['median_house_value'], prediction4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LPzgfVbaH-T2"
   },
   "source": [
    "It seems that the engineered dataset degrades the performance of Random forest regression! This is very unlikely since Random forest regression is more powerful than the linear regression. Is it possible to tune the parameter of a Machine Learning algorithm to achieve better accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kfjYU7CRyAP1"
   },
   "source": [
    "## Recap\n",
    "\n",
    "We have just demonstrated how to carry out a Machine Learning project on a given dataset. Specifically, a multivariate regression task in a supervised model-based batch learning framework. We have shown that a dataset needs to be properly inspected and some data cleaning techniques performed before applying any Machine Learning algorithm. Significant improvement can be obtained by not only changing the Machine Learning algorithm but combining it with feature engineering. There are a number of things that we have not covered here, but you can learn them by trying our exercises below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "apRVdL1UyAP2"
   },
   "source": [
    "## Exercises (it's OK if you can't complete them all today)\n",
    "\n",
    "1. We discussed earlier the difference between RSME and MAE but What can be implied if RMSE is significantly higher than MAE? Is it true for this house prediction problem? (You can try compute the MAE in the cell below.)\n",
    "\n",
    "_Hint_: Since both RMSE and MAE is a measure to quantify error in regression task, we need to apply them in different type of numerical predictions that our Machine Learning model can make in order to see their difference. Try the following.\n",
    "- Create a vector of 1000 or more numerical values. (These can be the same value or different values.) We will treat this vector's as true target values that our Machine Learning model needs to predict as accurate as possible.\n",
    "- Generate a vector of the same size where this new vector's values has a normal (Gaussian) distribution with zero mean and a unit variance. (You can use numpy.random.normal().) Visualise the distribution of these values with a histogram. We will treat them as errors that our Machine Learning model make.\n",
    "- Duplicate the true target values and apply this Guassian errors to them simply by pairwise summing.\n",
    "- Compute RMSE and MAE from the noisy prediction and the true target values.\n",
    "- Repeat the same process by trying different means and/or variances of the Gaussian noise and then compute RMSE & MAE. You can also try a uniformly random noise as well. What did you see when you have errors with a non-zero mean and/or a large variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 629
    },
    "colab_type": "code",
    "id": "pRqJyvDXyAP5",
    "outputId": "04f04225-6f45-47d3-830a-5c3e3d72b12b"
   },
   "outputs": [],
   "source": [
    "# Solution 1: \n",
    "# We can see from figures below that, in general, MAE is always no larger than \n",
    "# RMSE. Moreover, the higher the variance of error, the higher value a \n",
    "# difference between MAE and RMSE is. The source of high variance can come from\n",
    "# either (a) the target values are corrupted with noise, or (b) our model fails\n",
    "# to capture a detail in prediction's trend with respect to some attributes.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# function to compute RMSE\n",
    "def rmse(target, prediction):\n",
    "    return np.sqrt(np.mean(np.square(target - prediction)))\n",
    "\n",
    "# function to compute MAE\n",
    "def mae(target, prediction):\n",
    "    return np.mean(np.absolute(target - prediction))\n",
    "\n",
    "# generate numbers running from 0 to 1\n",
    "target = np.linspace(0, 1, num=1000)\n",
    "\n",
    "# generate prediction error with different means and different variances\n",
    "errors = [np.random.normal(loc=0.0, scale=1.0, size=1000), \n",
    "        np.random.normal(loc=100.0, scale=1.0, size=1000), \n",
    "        np.random.normal(loc=0.0, scale=100.0, size=1000), \n",
    "        np.random.normal(loc=100.0, scale=100.0, size=1000)]\n",
    "\n",
    "# indices for plotting histograms of error\n",
    "idx = [221, 222, 223, 224]\n",
    "\n",
    "\n",
    "#prediction = []\n",
    "for i in range(len(idx)):\n",
    "  \n",
    "    # Plotting histogram of errors\n",
    "    plt.subplot(idx[i])\n",
    "    plt.hist(errors[i])\n",
    "    plt.title(\"Error \" + str(i))\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    \n",
    "    # Compute predictions, RMSE, MAE, and their difference\n",
    "    prediction = target + errors[i]\n",
    "    rmse_val = rmse(target, prediction)\n",
    "    mae_val = mae(target, prediction)\n",
    "    print(\"RMSE\", i, \":\", rmse_val)\n",
    "    print(\"MAE\", i, \":\", mae_val)\n",
    "    print(\"Difference :\", rmse_val - mae_val)\n",
    "    print()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GB6aU4AWyAP7"
   },
   "source": [
    "2. Instead of dropping some datapoints that have missing values, we can try and fill them with a median of that attributes. Will the performance measure increase?\n",
    "\n",
    "_Hint_: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html\n",
    "\n",
    "_Extra hint_: Do you think that a median is always an appropriate value to use? What's about a mean or a mode? Which one would be appropriate for normally (Gaussian) distributed attribute? Which one would be appropriate for log-normal distribution or long-tailed distribution? (Search for a figure of these distributions on Google. It's useful for you to know at least what the shape of these distributions are and how frequent we can expect for each value in the distribution's range.)\n",
    "\n",
    "_Extra extra hint_: Will our Machine Learning model always benefit from having these datapoints with filled attributes? How? What if the proportion of these datapoints is more than 50%? Also, what if the attribute is one of the main factors that affects the target value (e.g. number of rooms and a house price)? Will our Machine Learning model still be able to use and learn from such filled attributes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fy008uUlyAP8"
   },
   "outputs": [],
   "source": [
    "Solution 2: \n",
    "Given that the attribute with missing values is important to make a prediction, \n",
    "filling the attribute is better than dropping the attribute, and consequently \n",
    "the performance measure will generally improve. Note that dropping datapoints \n",
    "results in less data for Machine Learning algorithm to train.\n",
    "\n",
    "Additionally, other values such as a mean or a mode may even give a better \n",
    "performance than the median. A number of techniques can also be used to get \n",
    "values to fill. For further detail, have a quick look at this paper\n",
    "\n",
    "Kotsiantis, S.B., Kanellopoulos, D. and Pintelas, P.E., 2006. Data preprocessing\n",
    "for supervised leaning. International Journal of Computer Science, 1(2), \n",
    "pp.111-117."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "quJ6hGfpyAP-"
   },
   "source": [
    "3. What is the consequence of ignoring the datapoints with a capped value in our dataset?\n",
    "\n",
    "_Hint_: By discarding those datapoints, you won't have them to let the Machine Learning model learn. When you roll out your Machine Learning project, what will then happen with a prediciton for inputs with those capped values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1KUH4z_MyAP_"
   },
   "outputs": [],
   "source": [
    "Solution 3: \n",
    "Our prediction that bases on data with capped value is not reliable. Such a \n",
    "prediction is an extrapolation which must be treated carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bo8JgGVKyAQC"
   },
   "source": [
    "4. Instead of encoding a categorical attribute into a number of binary attributes, will our performance measure increase if we encode it into one attribute with each value representing one category? What could be a reason for such improvement?\n",
    "\n",
    "_Hint_: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.factorize.html\n",
    "\n",
    "_Extra hint_: What is the difference between ordinal data and categorical data? Can categorical data always be ordered according to some metrics? What will then be a consequence of coding categorical data into a numerical value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "docA6JuCyAQD"
   },
   "outputs": [],
   "source": [
    "Solution 4: \n",
    "Typically, such encoding is not appropriate for categorical attributes and \n",
    "consequently should not improve the performance of Machine Learning models. \n",
    "In particular, encoding such attributes into a numerical attribute will also \n",
    "embed the quantitative difference among each pair of categories which is usually\n",
    "not true. Even if there is such a difference, the encoding has to preserve the \n",
    "original quantitative value as much as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mbTAl-rNyAQF"
   },
   "source": [
    "5. In many dataset including ours, different attributes have different ranges of value. Whilst our Machine Learning algorithm can cope with this issue to certain degree, it is widely known that either standardisation or normalisation should be applied. Try them separately on some attributes in our dataset, and observe any change in the performance measure.\n",
    "\n",
    "_Hint_: http://scikit-learn.org/stable/modules/preprocessing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NTiw2C3RyAQF"
   },
   "outputs": [],
   "source": [
    "# Solution 5\n",
    "from sklearn import preprocessing\n",
    "\n",
    "scaled_train_features = preprocessing.scale(train_features)\n",
    "# Other types of preprocessing can be looked in the hint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iAZSrTvdyAQG"
   },
   "source": [
    "6. We had randomly partitioned the dataset into the train set and the test set. It might be the case that we were lucky and randomly chose a test set that yielded a very low RMSE. To properly evaluate performance of our Machine Learning algorithm, you should try using all datapoints in your dataset as a test set and make sure that the RMSE is significatly low. This is called 'Cross-Validation.' Try it with our dataset and one Machine Learning model.\n",
    "\n",
    "_Hint_: http://scikit-learn.org/stable/modules/cross_validation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fhvTCQh1yAQI"
   },
   "outputs": [],
   "source": [
    "# Solution 6\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "reg = RandomForestRegressor()\n",
    "scores = cross_val_score(reg, features, target, cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gN6Y-ZY3yAQJ"
   },
   "source": [
    "7. Many Machine Learning algorithms including the Random Forest Regression have a number of parameters to tune. Try tuning our Random Forest Regressor so that it achieves the lowest RMSE.\n",
    "\n",
    "_Hint_: Instead of manually tuning these parameters, you can try a search such as a grid search (http://scikit-learn.org/stable/modules/grid_search.html) to find a good combination of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hTY0_46PyAQL"
   },
   "outputs": [],
   "source": [
    "# Solution 7\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "param_grid = [{'n_estimators': [1, 3, 5, 7, 9, 11, 13, 15, 17, 19], 'max_depth': [10, 20, 30, 40, 50]}]\n",
    "\n",
    "cv = GridSearchCV(RandomForestRegressor(), param_grid)\n",
    "cv.fit(train_features, train_label)\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print(clf.best_params_)\n",
    "\n",
    "# Detailed example of using GridSearchCV is accessible from \n",
    "# http://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html#sphx-glr-auto-examples-model-selection-plot-grid-search-digits-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Uo1eaOzyAQO"
   },
   "source": [
    "8. In practice, after training your first prototype, you are likely to acquire new datapoints or update your existing datapoints. How can you utilise them to improve your Machine Learning algorithm?\n",
    "\n",
    "_Hint_: Do those new datapoints significantly different to the previous one? If they do, you need to make your Machine Learning algorithm adapt to these new datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j-y4DJdPyAQO"
   },
   "outputs": [],
   "source": [
    "Solution 8: \n",
    "Given that a training of the Machine Learning model is not computationally \n",
    "expensive and not time-consuming, the new data can be incorporated into the \n",
    "dataset and the model can be trained freshly from the beginning. \n",
    "\n",
    "Contrarily, an online learning is required if the model is not easy to train. \n",
    "The key idea of an online learning is that an online training method of the \n",
    "Machine Learning model will learn from one datapoint or a small batch of data \n",
    "at each training epoch. Such a training method is different to the offline \n",
    "method and generally guarantees a well trained model given some conditions. \n",
    "\n",
    "For further information, have a quick look on the following sources:\n",
    "\n",
    "Wikipedia contributors, 'Online machine learning', Wikipedia, The Free \n",
    "Encyclopedia, 27 September 2018, 05:30 UTC, <https://en.wikipedia.org/w/index.php?title=Online_machine_learning&oldid=861407129> \n",
    "\n",
    "Shalev-Shwartz, S., 2012. Online learning and online convex optimization. \n",
    "Foundations and Trends® in Machine Learning, 4(2), pp.107-194. <http://www.cs.huji.ac.il/~shais/papers/OLsurvey.pdf>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lab1-solution.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
