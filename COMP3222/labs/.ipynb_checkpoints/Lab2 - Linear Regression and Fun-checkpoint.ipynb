{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "zFcKVBEd0gxn",
    "outputId": "18f57e65-cf9e-49ff-f874-6454c593270b"
   },
   "outputs": [],
   "source": [
    "#Our goal is to learn how to solve linear regression problems and understand the process thoroughly.\n",
    "#In Python, we import the libraries that will help us do rest of the tasks\n",
    "\n",
    "#If you are too bold and want do not want to follow me, solve the following:\n",
    "#Download the dataset and read the problem here:https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/overview. Try and develop an algorithm to solve the problem.\n",
    "#While you could find solved codes in Python for the problem, it does not help if you borrow someone else's ideas.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "#let's generate some 20 random numbers. You can play with these functions too to see what works and what does not. Even change the random numbers to generate numbers from a particular distribution, e.g., Gaussian\n",
    "\n",
    "x = random.sample(range(1, 200), 20) #randomly generate 20 random numbers\n",
    "y = random.sample(range(1, 500), 20) #randomly generate 20 random numbers\n",
    "print(x)\n",
    "print(y)\n",
    "plt.scatter(x, y)\n",
    "plt.show()\n",
    "\n",
    "#You must read here  to understand the function below: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html\n",
    "slope, intercept, r, p, std_err = stats.linregress(x, y)\n",
    "print(slope)\n",
    "\n",
    "#Remeber W.x+b\n",
    "def Regressor(x):\n",
    "  return slope * x + intercept\n",
    "\n",
    "#Create a list.\n",
    "LinearRegression = list(map(Regressor, x))\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, LinearRegression)\n",
    "plt.show()\n",
    "\n",
    "#Now, lets try and fix the problem of points above.\n",
    "rng = np.random.default_rng()\n",
    "x = rng.random(20)\n",
    "y = 1.6*x + rng.random(20)\n",
    "\n",
    "Residuals = stats.linregress(x, y)\n",
    "plt.plot(x, y, 'o', label='original data')\n",
    "plt.plot(x, Residuals.intercept + Residuals.slope*x, 'r', label='fitted line')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Let's try some non-linear fitting\n",
    "#The function below loosely follows a Normal/Gaussian Distribution\n",
    "#Credit: https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html\n",
    "def NormalFunction(x, a, b, c):\n",
    "    return a * np.exp(-b * x) + c\n",
    "\n",
    "x = np.linspace(0, 4, 50)\n",
    "y = NormalFunction(x, 2.5, 1.3, 0.5)\n",
    "rng = np.random.default_rng()\n",
    "y_noise = 0.2 * rng.normal(size=x.size)\n",
    "ydata = y + y_noise\n",
    "plt.plot(x, ydata, 'b-', label='data')\n",
    "\n",
    "popt, pcov = curve_fit(NormalFunction, x, ydata)\n",
    "popt, pcov = curve_fit(NormalFunction, x, ydata, bounds=(0, [3., 1., 0.5]))\n",
    "plt.plot(x, NormalFunction(x, *popt), 'g--',\n",
    "         label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#TODO:\n",
    "#1. Learn how to do train and test splitting. Reference: https://www.w3schools.com/python/python_ml_train_test.asp\n",
    "#2. Learn how to address overfitting problem. Reference: https://medium.datadriveninvestor.com/the-problem-of-overfitting-in-regression-and-how-to-avoid-it-dac4d49d836f\n",
    "#3. Learn how to address underfitting problem. Reference: https://itnext.io/linear-regression-how-to-overcome-underfitting-with-locally-weight-linear-regression-lwlr-e867f0cde4a4\n",
    "\n",
    "#Automatically generated code (Google Bard):\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Generate some noisy data\n",
    "x = np.random.rand(100, 1)\n",
    "y = 2 * x + 3 + np.random.rand(100, 1)\n",
    "\n",
    "# Create a polynomial feature transformer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=100)\n",
    "\n",
    "# Transform the data\n",
    "X_poly = poly.fit_transform(x)\n",
    "\n",
    "# Train a linear regression model on the transformed data\n",
    "reg = LinearRegression().fit(X_poly, y)\n",
    "\n",
    "# Plot the original data and the overfitted line\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, reg.predict(X_poly))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
